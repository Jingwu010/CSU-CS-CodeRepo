{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}\\;}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}\\;}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will develop a probabilistic model of data using a Gaussian distribution in such a way that we arrive at exactly the same optimal weight values as before.  In the following picture\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/regression-gaussian.png\">\n",
    "\n",
    "we see data points that appear to have a linear relationship that might be modeled well with a Gaussian density function having a mean that depends linearly on $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget the red line for a minute.  We want to model the joint probability, $p(x,t)$, of the data points.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/regression-gaussian-points.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember your joint probability definitions?  $p(x,t) = p(x|t)p(t) = p(t|x)p(x)$. Think of the second form as first finding the $p(x)$, then defining $p(t|x)$.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs445/notebooks/regression-gaussian-conditional.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we model $p(t|x)$? Let's assume samples tend to be near a certain value, and probability trails off to zero for values further away. What does that sound like? Yep, Gaussians!\n",
    "\n",
    "$$p(t|x) = \\Norm(t; \\mu, \\sigma^2)$$\n",
    "\n",
    "But, what are $\\mu$ and $\\sigma$? A simple (and common) assumption is $\\mu = g(\\xv;\\wv) = \\xv^T \\wv$ and $\\sigma$ is a constant. For now, we will leave $\\mu = g(\\xv;\\wv)$ without making the linearity assumption.  So\n",
    "\n",
    "$$p(x,t) = \\Norm(t; g(\\xv;\\wv), \\sigma^2) p(\\xv)$$\n",
    "\n",
    "To model a set of $N$ samples $\\{(\\xv_i,\\tv_i) \\;|\\; i=1,\\ldots,N\\}$ we want the best values of parameters $\\wv$. What is best? We want resulting probabilities that best match the likelihood of the observed data. Likelihood of all data samples is the product of the probabilties of each sample. (Why a product?)\n",
    "\n",
    "$$L(\\Xv,\\Tv;\\wv) = \\prod_{i=1}^N p(\\xv_i,t_i; \\wv)$$\n",
    "\n",
    "Best $\\wv$ is one that maximizes this. Observed data must be predicted to be very likely to occur.\n",
    "\n",
    "How can we maximize this?  Right! Take its derivative with respect to $\\wv$, set equal to zero, and sovle for $\\wv$.\n",
    "\n",
    "What is the derivative of a product?  Horrible!  Easier to take derivatives of sum.  Again, logarithms to the rescue. Assume $\\log$ is the natural logarithm (base $e$).\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\argmax{a} f(a) &= \\argmax{a} \\log f(a)\\\\\n",
    "\\log \\prod_i f(a_i) &= \\sum_i \\log f(a_i)\\\\\n",
    "\\argmax{w} \\prod_i f(a_i;w) &= \\argmax{w} \\log \\prod_i f(a_i;w) \\\\\n",
    "&= \\argmax{w} \\sum_i \\log f(a_i;w)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\wv^* &= \\argmax{\\wv} L(\\Xv,\\Tv;\\wv)\\\\\n",
    "&= \\argmax{\\wv} \\log L(\\Xv,\\Tv;\\wv)\\\\\n",
    "&= \\argmax{\\wv} \\log \\prod_{i=1}^N p(\\xv_i,t_i;\\wv)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N \\log p(\\xv_i,t_i;\\wv)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N \\log p(t_i \\;|\\; \\xv_i, \\wv) p(\\xv_i)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N \\log p(t_i \\;|\\; \\xv_i, \\wv) + \\log p(\\xv_i)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N \\log \\Norm(t_i; g(\\xv_i;\\wv),\\sigma^2) + \\log p(\\xv_i)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N \\log \\frac{1}{\\sqrt(2\\pi)\\sigma} e^{-\\frac{1}{2\\sigma} (t_i - \\xv_i;\\wv))^2} + \\log p(\\xv_i)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N \\log \\frac{1}{\\sqrt(2\\pi)\\sigma} + \\log e^{-\\frac{1}{2\\sigma} (t_i - g(\\xv_i;\\wv))^2} + \\log p(\\xv_i)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N 0 - \\log (\\sqrt(2\\pi)\\sigma) + \\log e^{-\\frac{1}{2\\sigma} (t_i - g(\\xv_i;\\wv))^2} + \\log p(\\xv_i)\\\\\n",
    "&= \\argmax{\\wv} \\sum_{i=1}^N  - \\frac{1}{2} \\log(2\\pi\\sigma) - \\frac{1}{2\\sigma} (t_i - g(\\xv_i;\\wv))^2 + \\log p(\\xv_i)\\\\\n",
    "&= \\argmin{\\wv} \\sum_{i=1}^N \\frac{1}{2} \\log(2\\pi\\sigma)+ \\frac{1}{2\\sigma} (t_i- g(\\xv_i;\\wv))^2 - \\log p(\\xv_i)\\\\\n",
    "&= \\argmin{\\wv} \\sum_{i=1}^N (t_i - g(\\xv_i;\\wv))^2 \n",
    "\\end{align*}$$\n",
    "\n",
    "which should look very familiar!  It is exactly the same sum of squared errors objective function we had before, when we were thinking about springs. Our solution is the same as before:\n",
    "\n",
    "$$\\wv = (\\Xv^T \\Xv)^{-1} \\Xv^T T$$\n",
    "\n",
    "Where did $\\sigma$ go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, what is the take away message of these notes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
